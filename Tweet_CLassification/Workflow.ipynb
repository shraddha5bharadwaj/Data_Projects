{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"something.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"JJpwzHlgqHAe"},"source":["# Content Words"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UxXfvKAm2ROc","executionInfo":{"status":"ok","timestamp":1619116248973,"user_tz":-330,"elapsed":6808,"user":{"displayName":"shraddha bharadwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhDNQt9tAYdGTonkIlvAJWWwl57-aaTNZ2JOU6Mg=s64","userId":"13889782027816332292"}},"outputId":"7e2c2f86-9a00-43db-f5fe-880ae311aea7"},"source":["!pip install pymprog "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting pymprog\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/b4/27e6dc89387535829127b8e577226f0a82215da174b451198312e0afe5d0/pymprog-1.1.2.tar.gz (44kB)\n","\u001b[K     |████████████████████████████████| 51kB 2.6MB/s \n","\u001b[?25hCollecting swiglpk>=1.4.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/48/ff3ce61567f667629268b272d4c57f980ff0a8d4bdf991a593be12384186/swiglpk-5.0.3-cp37-cp37m-manylinux2010_x86_64.whl (2.1MB)\n","\u001b[K     |████████████████████████████████| 2.1MB 5.8MB/s \n","\u001b[?25hBuilding wheels for collected packages: pymprog\n","  Building wheel for pymprog (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pymprog: filename=pymprog-1.1.2-cp37-none-any.whl size=43502 sha256=44c78b4dd67cd741f1b71cb4fca033c18310fbccf87024ad3304732f1ec1e41e\n","  Stored in directory: /root/.cache/pip/wheels/fa/09/59/ef649891969b65ff8de663e7a0a65bf27ba0c273e2a0795a44\n","Successfully built pymprog\n","Installing collected packages: swiglpk, pymprog\n","Successfully installed pymprog-1.1.2 swiglpk-5.0.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gXr3yRFn4GxS","executionInfo":{"status":"ok","timestamp":1619118797470,"user_tz":-330,"elapsed":18363,"user":{"displayName":"shraddha bharadwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhDNQt9tAYdGTonkIlvAJWWwl57-aaTNZ2JOU6Mg=s64","userId":"13889782027816332292"}},"outputId":"2d07c75a-c06e-4c22-bf32-ec42a6b0cf88"},"source":["!pip install nltk\n","\n","!pip install spacy==2.3.5\n","\n","!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz\n","\n","!pip install pyresparser"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.6.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.41.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk) (2019.12.20)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n","Requirement already satisfied: spacy==2.3.5 in /usr/local/lib/python3.7/dist-packages (2.3.5)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (0.8.2)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (1.0.5)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (1.0.5)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (4.41.1)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (1.1.3)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (1.19.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (56.0.0)\n","Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (7.4.5)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (0.4.1)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (2.0.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (2.23.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (3.0.5)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (1.0.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (1.25.11)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (2020.12.5)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.3.5) (3.10.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy==2.3.5) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy==2.3.5) (3.7.4.3)\n","Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz\n","\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0MB)\n","\u001b[K     |████████████████████████████████| 12.1MB 324kB/s \n","\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): en-core-web-sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz in /usr/local/lib/python3.7/dist-packages\n","Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==2.3.1) (2.3.5)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (1.0.5)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (4.41.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (1.19.5)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (0.8.2)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (1.0.5)\n","Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (7.4.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (2.23.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (3.0.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (56.0.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (1.1.3)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (1.0.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (2.0.5)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (0.4.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (1.25.11)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (3.0.4)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (3.10.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (3.7.4.3)\n","Building wheels for collected packages: en-core-web-sm\n","  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.3.1-cp37-none-any.whl size=12047109 sha256=51e14b5dcdf9e3dcce20c0534cf1470803d7c9a05e7aaa1c996e7b14ad2d2bd3\n","  Stored in directory: /root/.cache/pip/wheels/2b/3f/41/f0b92863355c3ba34bb32b37d8a0c662959da0058202094f46\n","Successfully built en-core-web-sm\n","Requirement already satisfied: pyresparser in /usr/local/lib/python3.7/dist-packages (1.0.6)\n","Requirement already satisfied: urllib3>=1.25.3 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (1.25.11)\n","Requirement already satisfied: pyrsistent>=0.15.2 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (0.17.3)\n","Requirement already satisfied: spacy>=2.1.4 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (2.3.5)\n","Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (2.23.0)\n","Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (1.1.5)\n","Requirement already satisfied: tqdm>=4.32.2 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (4.41.1)\n","Requirement already satisfied: cymem>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (2.0.5)\n","Requirement already satisfied: pycryptodome>=3.8.2 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (3.10.1)\n","Requirement already satisfied: pdfminer.six>=20181108 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (20201018)\n","Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (2.8.1)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (2.10)\n","Requirement already satisfied: pytz>=2019.1 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (2021.1)\n","Requirement already satisfied: nltk>=3.4.3 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (3.6.2)\n","Requirement already satisfied: certifi>=2019.6.16 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (2020.12.5)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (20.3.0)\n","Requirement already satisfied: sortedcontainers>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (2.3.0)\n","Requirement already satisfied: wasabi>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (0.8.2)\n","Requirement already satisfied: srsly>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (1.0.5)\n","Requirement already satisfied: blis>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (0.4.1)\n","Requirement already satisfied: docx2txt>=0.7 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (0.8)\n","Requirement already satisfied: preshed>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (3.0.5)\n","Requirement already satisfied: chardet>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (3.0.4)\n","Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (1.19.5)\n","Requirement already satisfied: jsonschema>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (3.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (1.15.0)\n","Requirement already satisfied: thinc>=7.0.4 in /usr/local/lib/python3.7/dist-packages (from pyresparser) (7.4.5)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.4->pyresparser) (1.0.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.4->pyresparser) (1.1.3)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.4->pyresparser) (1.0.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.4->pyresparser) (56.0.0)\n","Requirement already satisfied: cryptography in /usr/local/lib/python3.7/dist-packages (from pdfminer.six>=20181108->pyresparser) (3.4.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.4.3->pyresparser) (1.0.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk>=3.4.3->pyresparser) (2019.12.20)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.4.3->pyresparser) (7.1.2)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->pyresparser) (3.10.1)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six>=20181108->pyresparser) (1.14.5)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema>=3.0.1->pyresparser) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema>=3.0.1->pyresparser) (3.7.4.3)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six>=20181108->pyresparser) (2.20)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YJp2_u3ukxrS"},"source":["\n","import spacy\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jD_XwcvhqHA8"},"source":["\n","from __future__ import division\n","\n","\n","\n","import numpy as np \n","import pandas as pd \n","import spacy\n","from spacy.tokens.doc import Doc\n","import inspect\n","#import textacy \n","\n","#from textacy.vsm import Vectorizer\n","#import textacy.vsm\n","\n","\n","import scipy.sparse as sp\n","import tqdm as tqdm\n","\n","import re"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LFKqvZlUkxri"},"source":["%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7LXSd8b7ljU4","executionInfo":{"status":"ok","timestamp":1619270383882,"user_tz":-330,"elapsed":58942,"user":{"displayName":"shraddha bharadwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhDNQt9tAYdGTonkIlvAJWWwl57-aaTNZ2JOU6Mg=s64","userId":"13889782027816332292"}},"outputId":"1c162e10-5d12-4f2a-f6ac-709b519a0d95"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KlmvZM91qHBA"},"source":["Loading the data"]},{"cell_type":"code","metadata":{"id":"syFPpdryvYRs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619270403744,"user_tz":-330,"elapsed":1719,"user":{"displayName":"shraddha bharadwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhDNQt9tAYdGTonkIlvAJWWwl57-aaTNZ2JOU6Mg=s64","userId":"13889782027816332292"}},"outputId":"3e81c174-e741-4eb1-ea13-4e5fda3dbdd8"},"source":["train = pd.read_csv('/content/drive/My Drive/AIR/train.csv', encoding = 'ISO-8859-1')\n","test = pd.read_csv('/content/drive/My Drive/AIR/test.csv', encoding = 'ISO-8859-1')\n","print(len(train),len(test))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["7613 3263\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O86wlrLQkxrp","executionInfo":{"status":"ok","timestamp":1619270440830,"user_tz":-330,"elapsed":1029,"user":{"displayName":"shraddha bharadwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhDNQt9tAYdGTonkIlvAJWWwl57-aaTNZ2JOU6Mg=s64","userId":"13889782027816332292"}},"outputId":"dbc58eb3-51db-49d2-a4d4-3e46075ee500"},"source":["for col in train.columns:\n","        print(col , train[col].isna().sum())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["id 0\n","keyword 61\n","location 2533\n","text 0\n","target 0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uzPt5ri7kxrr","executionInfo":{"status":"ok","timestamp":1619270443675,"user_tz":-330,"elapsed":965,"user":{"displayName":"shraddha bharadwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhDNQt9tAYdGTonkIlvAJWWwl57-aaTNZ2JOU6Mg=s64","userId":"13889782027816332292"}},"outputId":"b35a7f1e-a80b-4405-e479-71ba8b3ad38f"},"source":["for col in test.columns:\n","        print(col , test[col].isna().sum())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["id 0\n","keyword 26\n","location 1105\n","text 0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ur38zpmvqHBD","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1619270445805,"user_tz":-330,"elapsed":1166,"user":{"displayName":"shraddha bharadwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhDNQt9tAYdGTonkIlvAJWWwl57-aaTNZ2JOU6Mg=s64","userId":"13889782027816332292"}},"outputId":"b9dd22c2-e95d-47c7-dc4e-97ff6a4589f3"},"source":["train.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Our Deeds are the Reason of this #earthquake M...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Forest fire near La Ronge Sask. Canada</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>All residents asked to 'shelter in place' are ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>13,000 people receive #wildfires evacuation or...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id keyword  ...                                               text target\n","0   1     NaN  ...  Our Deeds are the Reason of this #earthquake M...      1\n","1   4     NaN  ...             Forest fire near La Ronge Sask. Canada      1\n","2   5     NaN  ...  All residents asked to 'shelter in place' are ...      1\n","3   6     NaN  ...  13,000 people receive #wildfires evacuation or...      1\n","4   7     NaN  ...  Just got sent this photo from Ruby #Alaska as ...      1\n","\n","[5 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"6aa9NgBakxrx"},"source":["for df in [train , test]:\n","    for e in ['keyword', 'location']:\n","        df[e] = df[e].fillna('')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rFxainNhqHBH"},"source":["## Preprocessing\n","\n","For my tweets to be informative, there are a few terms I can immediately remove. For instance, any urls won't be useful to the rescue teams. Equally, any '@...' are just calling another twitter handle, and are equally not useful. "]},{"cell_type":"code","metadata":{"id":"YdjsOH9mkxr1"},"source":["def Clean(text):\n","    \n","    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n","    text = re.sub(r'http?:\\/\\/.*[\\r\\n]*', '', text) #remove URL\n","    \n","    text=text.replace(r'&amp;?',r'and')\n","    text=text.replace(r'&lt;',r'<')\n","    text=text.replace(r'&gt;',r'>') #remove special character \n","    \n","    text = re.sub(r\"(?:\\@)\\w+\", '', text)\n","    text=text.encode(\"ascii\",errors=\"ignore\").decode()\n","    text=re.sub(r'[:\"#$%&\\*+,-/:;<=>@\\\\^_`{|}~]+','',text)\n","    \n","    text=re.sub(r'[!]+','!',text)\n","    text=re.sub(r'[?]+','?',text)\n","    text=re.sub(r'[.]+','.',text)\n","   \n","    text=re.sub(r\"'\",\"\",text)\n","    text=re.sub(r\"\\(\",\"\",text)\n","    text=re.sub(r\"\\)\",\"\",text)\n","    \n","    text=\" \".join(text.split())\n","    return text\n","\n","def stemmer(text):\n","    stem_text = [PorterStemmer().stem(i) for i in text]\n","    return stem_text\n","\n","def lemmatizer(text):\n","    lem_text = [WordNetLemmatizer().lemmatize(i) for i in text]\n","    return lem_text\n","\n","train['text']=train['text'].apply(lambda x: Clean(x))\n","test['text']=test['text'].apply(lambda x: Clean(x))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GzKlwQlmqHBJ","colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"status":"ok","timestamp":1619270455981,"user_tz":-330,"elapsed":1487,"user":{"displayName":"shraddha bharadwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhDNQt9tAYdGTonkIlvAJWWwl57-aaTNZ2JOU6Mg=s64","userId":"13889782027816332292"}},"outputId":"352bd524-d5de-450f-f0c3-d0e919b47268"},"source":["train.head(-5)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td></td>\n","      <td></td>\n","      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4</td>\n","      <td></td>\n","      <td></td>\n","      <td>Forest fire near La Ronge Sask Canada</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5</td>\n","      <td></td>\n","      <td></td>\n","      <td>All residents asked to shelter in place are be...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6</td>\n","      <td></td>\n","      <td></td>\n","      <td>13000 people receive wildfires evacuation orde...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7</td>\n","      <td></td>\n","      <td></td>\n","      <td>Just got sent this photo from Ruby Alaska as s...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7603</th>\n","      <td>10862</td>\n","      <td></td>\n","      <td></td>\n","      <td>Officials say a quarantine is in place at an A...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7604</th>\n","      <td>10863</td>\n","      <td></td>\n","      <td></td>\n","      <td>WorldNews Fallen powerlines on Glink tram UPDA...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7605</th>\n","      <td>10864</td>\n","      <td></td>\n","      <td></td>\n","      <td>on the flip side Im at Walmart and there is a ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7606</th>\n","      <td>10866</td>\n","      <td></td>\n","      <td></td>\n","      <td>Suicide bomber kills 15 in Saudi security site...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7607</th>\n","      <td>10867</td>\n","      <td></td>\n","      <td></td>\n","      <td>stormchase Violent Record Breaking EF5 El Reno...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7608 rows × 5 columns</p>\n","</div>"],"text/plain":["         id keyword  ...                                               text target\n","0         1          ...  Our Deeds are the Reason of this earthquake Ma...      1\n","1         4          ...              Forest fire near La Ronge Sask Canada      1\n","2         5          ...  All residents asked to shelter in place are be...      1\n","3         6          ...  13000 people receive wildfires evacuation orde...      1\n","4         7          ...  Just got sent this photo from Ruby Alaska as s...      1\n","...     ...     ...  ...                                                ...    ...\n","7603  10862          ...  Officials say a quarantine is in place at an A...      1\n","7604  10863          ...  WorldNews Fallen powerlines on Glink tram UPDA...      1\n","7605  10864          ...  on the flip side Im at Walmart and there is a ...      1\n","7606  10866          ...  Suicide bomber kills 15 in Saudi security site...      1\n","7607  10867          ...  stormchase Violent Record Breaking EF5 El Reno...      1\n","\n","[7608 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"5RrrasESkxr4","executionInfo":{"status":"ok","timestamp":1619270459028,"user_tz":-330,"elapsed":722,"user":{"displayName":"shraddha bharadwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhDNQt9tAYdGTonkIlvAJWWwl57-aaTNZ2JOU6Mg=s64","userId":"13889782027816332292"}},"outputId":"d2740423-a24a-4bf0-bbec-9e464d7e0865"},"source":["test.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td></td>\n","      <td></td>\n","      <td>Just happened a terrible car crash</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td></td>\n","      <td></td>\n","      <td>Heard about earthquake is different cities sta...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td></td>\n","      <td></td>\n","      <td>there is a forest fire at spot pond geese are ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>9</td>\n","      <td></td>\n","      <td></td>\n","      <td>Apocalypse lighting Spokane wildfires</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>11</td>\n","      <td></td>\n","      <td></td>\n","      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id keyword location                                               text\n","0   0                                  Just happened a terrible car crash\n","1   2                   Heard about earthquake is different cities sta...\n","2   3                   there is a forest fire at spot pond geese are ...\n","3   9                               Apocalypse lighting Spokane wildfires\n","4  11                       Typhoon Soudelor kills 28 in China and Taiwan"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"JwdQ8nR7qHBK","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1619270462262,"user_tz":-330,"elapsed":1118,"user":{"displayName":"shraddha bharadwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhDNQt9tAYdGTonkIlvAJWWwl57-aaTNZ2JOU6Mg=s64","userId":"13889782027816332292"}},"outputId":"bc62e1a6-5c2b-4fec-8f98-689d0277ec35"},"source":["data = [test[\"text\"]]\n","headers = [\"text\"]\n","tweets = pd.concat(data, axis=1, keys=headers)\n","tweets.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Just happened a terrible car crash</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Heard about earthquake is different cities sta...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>there is a forest fire at spot pond geese are ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Apocalypse lighting Spokane wildfires</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text\n","0                 Just happened a terrible car crash\n","1  Heard about earthquake is different cities sta...\n","2  there is a forest fire at spot pond geese are ...\n","3              Apocalypse lighting Spokane wildfires\n","4      Typhoon Soudelor kills 28 in China and Taiwan"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"Vh23a6z9qHBL"},"source":["There are alot of `u'RT'` terms in the tweet texts. Since this isn't a word, SpaCy doesn't know how to handle them. Since these add nothing to the content of a tweet, I'm just going to get rid of them. "]},{"cell_type":"code","metadata":{"collapsed":true,"id":"oydH5JJ1qHBN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619270465481,"user_tz":-330,"elapsed":999,"user":{"displayName":"shraddha bharadwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhDNQt9tAYdGTonkIlvAJWWwl57-aaTNZ2JOU6Mg=s64","userId":"13889782027816332292"}},"outputId":"32cd615a-703d-4778-86be-c18053cd9bf5"},"source":["len(tweets)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3263"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"913TTHaIqHBO"},"source":["Tokenizing with SpaCy"]},{"cell_type":"code","metadata":{"id":"Kz62_Y3qqHBO"},"source":["nlp = spacy.load('en_core_web_sm')#default pipeline package"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DLsJEFqkqHBP"},"source":["spacy_tweets = []\n","\n","for doc in nlp.pipe(tweets.text, n_threads = -1):\n","    spacy_tweets.append(doc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iRHjf3CzqHBP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619270569807,"user_tz":-330,"elapsed":1822,"user":{"displayName":"shraddha bharadwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhDNQt9tAYdGTonkIlvAJWWwl57-aaTNZ2JOU6Mg=s64","userId":"13889782027816332292"}},"outputId":"42fbb415-fcc6-4b63-80d9-4fac5473d5f8"},"source":["print(len(spacy_tweets))\n","spacy_tweets[1801]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["3263\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["Good morning Slept about 6 hours The heat wave hits Japan The hot days still continue I guess some people were killed by this climate"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"NIJFmc0EqHBQ"},"source":["Not all tweets are equally useful. Some just contain prayers, such as\n","\n","`Hope it doesn't rain. #Nepal`\n","\n","whereas others are dense with useful information: \n","\n","`2 Dead, 100 Injured in Bangladesh From Nepal Quake`"]},{"cell_type":"markdown","metadata":{"id":"wJK0B53LqHBR"},"source":["How do I decide which parts of these tweets are most useful? One way to do it is to measure the term frequency-inverse document frequency (tf-idf) of each of the words in the corpus of tweets. This metric measures how important a word is in a corpus of tweets. \n","\n","## Getting the tf-idf values of content words. \n","\n","I can do a preliminary 'cleanup', by keeping only 'content words'. These are defined as : Numerals, Nouns and Verbs. Conveniantly, SpaCy has already organised this for us. "]},{"cell_type":"code","metadata":{"id":"53QYQ85aqHBS"},"source":["spacy_tweets[90]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MYtZ8E8rqHBT"},"source":["I care most about tokens which are entities, and numbers. The other tokens have too much noise, so let's focus on these two:"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"ofRuMKqWqHBT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619117280877,"user_tz":-330,"elapsed":957,"user":{"displayName":"shraddha bharadwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhDNQt9tAYdGTonkIlvAJWWwl57-aaTNZ2JOU6Mg=s64","userId":"13889782027816332292"}},"outputId":"1ac110bf-9d80-455a-be72-745797f2a191"},"source":["target_1=train[train[\"target\"] == 1]\n","target_1=target_1[target_1[\"keyword\"] != '']\n","main_words = list(set(target_1[\"keyword\"]))\n","print(len(main_words))\n","main_words = ['\\\\'+\"u\"+s for s in main_words] #adding unicode escape sequence\n","main_words"],"execution_count":null,"outputs":[{"output_type":"stream","text":["220\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['\\\\uflood',\n"," '\\\\ubody%20bags',\n"," '\\\\uquarantine',\n"," '\\\\uburning%20buildings',\n"," '\\\\uweapon',\n"," '\\\\ubridge%20collapse',\n"," '\\\\usuicide%20bombing',\n"," '\\\\uscreamed',\n"," '\\\\ublood',\n"," '\\\\uelectrocuted',\n"," '\\\\urioting',\n"," '\\\\uattacked',\n"," '\\\\urescue',\n"," '\\\\uelectrocute',\n"," '\\\\utwister',\n"," '\\\\uruin',\n"," '\\\\ueyewitness',\n"," '\\\\ucyclone',\n"," '\\\\uloud%20bang',\n"," '\\\\ucatastrophic',\n"," '\\\\uwhirlwind',\n"," '\\\\uthunder',\n"," '\\\\unuclear%20reactor',\n"," '\\\\usnowstorm',\n"," '\\\\utraumatised',\n"," '\\\\uengulfed',\n"," '\\\\uhijack',\n"," '\\\\uoil%20spill',\n"," '\\\\uthunderstorm',\n"," '\\\\uwildfire',\n"," '\\\\ubattle',\n"," '\\\\ufatalities',\n"," '\\\\udisaster',\n"," '\\\\uemergency%20services',\n"," '\\\\ucliff%20fall',\n"," '\\\\udetonation',\n"," '\\\\uradiation%20emergency',\n"," '\\\\uexplosion',\n"," '\\\\uheat%20wave',\n"," '\\\\udevastation',\n"," '\\\\ublizzard',\n"," '\\\\udeluge',\n"," '\\\\urefugees',\n"," '\\\\ubloody',\n"," '\\\\uinjuries',\n"," '\\\\uwounded',\n"," '\\\\ubioterror',\n"," '\\\\uarmageddon',\n"," '\\\\uderailed',\n"," '\\\\uambulance',\n"," '\\\\ucatastrophe',\n"," '\\\\uhurricane',\n"," '\\\\ubush%20fires',\n"," '\\\\uhazardous',\n"," '\\\\udrought',\n"," '\\\\udust%20storm',\n"," '\\\\ucurfew',\n"," '\\\\ustructural%20failure',\n"," '\\\\usunk',\n"," '\\\\ubody%20bag',\n"," '\\\\uemergency%20plan',\n"," '\\\\udamage',\n"," '\\\\uattack',\n"," '\\\\ucrush',\n"," '\\\\ubody%20bagging',\n"," '\\\\ulightning',\n"," '\\\\uarson',\n"," '\\\\uhijacking',\n"," '\\\\uoutbreak',\n"," '\\\\uarmy',\n"," '\\\\ublew%20up',\n"," '\\\\ubombing',\n"," '\\\\uinundated',\n"," '\\\\ustorm',\n"," '\\\\uhostage',\n"," '\\\\uwar%20zone',\n"," '\\\\ucrushed',\n"," '\\\\udeath',\n"," '\\\\uhazard',\n"," '\\\\usirens',\n"," '\\\\usurvivors',\n"," '\\\\ucollapsed',\n"," '\\\\ufloods',\n"," '\\\\uterrorism',\n"," '\\\\ufamine',\n"," '\\\\udanger',\n"," '\\\\umeltdown',\n"," '\\\\usmoke',\n"," '\\\\uharm',\n"," '\\\\usandstorm',\n"," '\\\\uinjury',\n"," '\\\\udestruction',\n"," '\\\\udesolate',\n"," '\\\\unatural%20disaster',\n"," '\\\\urubble',\n"," '\\\\uannihilated',\n"," '\\\\uarsonist',\n"," '\\\\ucollapse',\n"," '\\\\uflattened',\n"," '\\\\uhailstorm',\n"," '\\\\uobliterate',\n"," '\\\\urescued',\n"," '\\\\uterrorist',\n"," '\\\\uthreat',\n"," '\\\\udrown',\n"," '\\\\ufatality',\n"," '\\\\usurvive',\n"," '\\\\ublown%20up',\n"," '\\\\udebris',\n"," '\\\\uevacuated',\n"," '\\\\umayhem',\n"," '\\\\ucollide',\n"," '\\\\ucasualty',\n"," '\\\\ublazing',\n"," '\\\\uchemical%20emergency',\n"," '\\\\uhellfire',\n"," '\\\\urescuers',\n"," '\\\\utrapped',\n"," '\\\\udemolished',\n"," '\\\\udestroy',\n"," '\\\\uobliterated',\n"," '\\\\uriot',\n"," '\\\\ubomb',\n"," '\\\\utrauma',\n"," '\\\\uairplane%20accident',\n"," '\\\\usinking',\n"," '\\\\ucrash',\n"," '\\\\ufire%20truck',\n"," '\\\\upandemonium',\n"," '\\\\uscreaming',\n"," '\\\\utsunami',\n"," '\\\\uwindstorm',\n"," '\\\\ucasualties',\n"," '\\\\ubuildings%20on%20fire',\n"," '\\\\udead',\n"," '\\\\udrowned',\n"," '\\\\udrowning',\n"," '\\\\ufear',\n"," '\\\\uflames',\n"," '\\\\ublight',\n"," '\\\\uvolcano',\n"," '\\\\uderail',\n"," '\\\\uaccident',\n"," '\\\\usinkhole',\n"," '\\\\utyphoon',\n"," '\\\\udevastated',\n"," '\\\\useismic',\n"," '\\\\uburned',\n"," '\\\\ubombed',\n"," '\\\\uapocalypse',\n"," '\\\\udesolation',\n"," '\\\\uavalanche',\n"," '\\\\uviolent%20storm',\n"," '\\\\ucollided',\n"," '\\\\ubleeding',\n"," '\\\\uexploded',\n"," '\\\\urainstorm',\n"," '\\\\uinundation',\n"," '\\\\uablaze',\n"," '\\\\uscreams',\n"," '\\\\umass%20murderer',\n"," '\\\\upolice',\n"," '\\\\utornado',\n"," '\\\\udisplaced',\n"," '\\\\ublaze',\n"," '\\\\upanicking',\n"," '\\\\uquarantined',\n"," '\\\\uwreckage',\n"," '\\\\urazed',\n"," '\\\\uwrecked',\n"," '\\\\ufatal',\n"," '\\\\umudslide',\n"," '\\\\uexplode',\n"," '\\\\ubioterrorism',\n"," '\\\\usuicide%20bomber',\n"," '\\\\uforest%20fire',\n"," '\\\\uhijacker',\n"," '\\\\ulandslide',\n"," '\\\\uobliteration',\n"," '\\\\ufire',\n"," '\\\\uevacuate',\n"," '\\\\ustretcher',\n"," '\\\\uearthquake',\n"," '\\\\uhail',\n"," '\\\\ulava',\n"," '\\\\uwounds',\n"," '\\\\uderailment',\n"," '\\\\utrouble',\n"," '\\\\ucollision',\n"," '\\\\udemolition',\n"," '\\\\udetonate',\n"," '\\\\umilitary',\n"," '\\\\uwreck',\n"," '\\\\umass%20murder',\n"," '\\\\uflooding',\n"," '\\\\uepicentre',\n"," '\\\\umassacre',\n"," '\\\\uwild%20fires',\n"," '\\\\ucrashed',\n"," '\\\\uforest%20fires',\n"," '\\\\usiren',\n"," '\\\\upanic',\n"," '\\\\uannihilation',\n"," '\\\\uinjured',\n"," '\\\\udemolish',\n"," '\\\\ufirst%20responders',\n"," '\\\\uupheaval',\n"," '\\\\udeaths',\n"," '\\\\uemergency',\n"," '\\\\usuicide%20bomb',\n"," '\\\\uweapons',\n"," '\\\\usurvived',\n"," '\\\\utragedy',\n"," '\\\\udeluged',\n"," '\\\\udestroyed',\n"," '\\\\unuclear%20disaster',\n"," '\\\\uevacuation',\n"," '\\\\ubuildings%20burning',\n"," '\\\\uhostages',\n"," '\\\\uburning']"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"collapsed":true,"id":"8CP8J_8iqHBU"},"source":["useful_entities = [u'NORP', u'FACILITY', u'ORG', u'GPE', u'LOC', u'EVENT', u'DATE', u'TIME']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9QkVG0wWqHBV","executionInfo":{"status":"ok","timestamp":1619117292027,"user_tz":-330,"elapsed":1448,"user":{"displayName":"shraddha bharadwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhDNQt9tAYdGTonkIlvAJWWwl57-aaTNZ2JOU6Mg=s64","userId":"13889782027816332292"}},"outputId":"969e393c-e671-4f4a-f887-cfe857c5201b"},"source":["content_tweets = []\n","for single_tweet in tqdm.tqdm(spacy_tweets):\n","    single_tweet_content = []\n","    for token in single_tweet: \n","        if ((token.ent_type_ in useful_entities)  \n","            or (token.pos_ == u'NUM') \n","            or (token.lower_ in main_words)):\n","            single_tweet_content.append(token)\n","    content_tweets.append(single_tweet_content)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 3263/3263 [00:00<00:00, 15033.15it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"fNqkjb0YqHBW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619117296733,"user_tz":-330,"elapsed":1358,"user":{"displayName":"shraddha bharadwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhDNQt9tAYdGTonkIlvAJWWwl57-aaTNZ2JOU6Mg=s64","userId":"13889782027816332292"}},"outputId":"89a6ab9e-ca13-40db-ef70-44a7f9a616a8"},"source":["tweet_num = 150\n","print (\"original_tweet \\n\" + str(spacy_tweets[tweet_num]) \n","       + \"\\n\\noriginal_tweet\\n\" + str([str(x) for x in spacy_tweets[tweet_num]])\n","       + \"\\n\\ncontent_tweet\\n\" + str(content_tweets[tweet_num])\n","      )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["original_tweet \n","You da One MTVSummerStar VideoVeranoMTV MTVHottest Britney Spears Lana Del Rey\n","\n","original_tweet\n","['You', 'da', 'One', 'MTVSummerStar', 'VideoVeranoMTV', 'MTVHottest', 'Britney', 'Spears', 'Lana', 'Del', 'Rey']\n","\n","content_tweet\n","[One]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qAnrAfKqqHBX"},"source":["So this has already gone some way to (crudely) isolating the interesting parts of a tweet. "]},{"cell_type":"markdown","metadata":{"id":"ybxaWR5fqHBY"},"source":["Unfortunately, SpaCy doesn't calculate tf-idf score automatically. There IS a library which can do this: [textacy](https://textacy.readthedocs.io/en/latest/index.html). Note: textacy is built on SpaCy.\n","\n","I care about the tf-idf scores of the entire tweet, so will find the tf-idf score across the entire corpus of original tweets. "]},{"cell_type":"code","metadata":{"id":"rdKw0d-XqHBZ","colab":{"base_uri":"https://localhost:8080/","height":163},"executionInfo":{"status":"error","timestamp":1619117301752,"user_tz":-330,"elapsed":1042,"user":{"displayName":"shraddha bharadwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhDNQt9tAYdGTonkIlvAJWWwl57-aaTNZ2JOU6Mg=s64","userId":"13889782027816332292"}},"outputId":"c9c29db8-6771-4fc5-aa09-73345bfbe5c7"},"source":["vectorizer = Vectorizer(weighting = 'tfidf')"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-70f0d2bdc0d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tfidf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'Vectorizer' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"8MVTChyEqHBa"},"source":["To calculate the tf-idf score of all the tokens in the tweets, I can use `fit_transform()`. \n","\n","Note: I am using the `lemma_` attribute of each token, because tokens contain information about the documents. This means that 'Nepal' in the 100th tweet will have a different **token** from 'Nepal' in the 200th tweet, but the same `lemma__` attribute. This is what I want to compare - I don't want hundreds of 'Nepal' columns in my term matrix. "]},{"cell_type":"code","metadata":{"id":"1tHN1V3wqHBb"},"source":["term_matrix = vectorizer.fit_transform([tok.lemma_ for tok in doc] for doc in spacy_tweets)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w4kRvAuqqHBc"},"source":["This matrix is a term-document matrix. What this means is that on top of having the tf-idf values, each row is a document (and each column is a word). \n","\n","If the tweet in row `i` contains the column in row `j`, then the element `matrix[i][j]` will contain the tf-idf value. If the tweet *doesn't* contan the word, the matrix value will be zero. "]},{"cell_type":"code","metadata":{"collapsed":true,"id":"xMA6DYuXqHBd"},"source":["np_matrix = term_matrix.todense()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ybFaSltbqHBd"},"source":["np_matrix.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"tVQwnqr_qHBe"},"source":["My ultimate goal is to create a dictionary, which maps from the tokens in the content tweets to some tf-idf score. To do this, I need to find out which tokens are at what columns in the term matrix. \n","\n","The vectorizer object has a dictionary, which maps token.lemma_ to its column. "]},{"cell_type":"code","metadata":{"id":"7MTd3J3MqHBg"},"source":["for key in sorted(vectorizer.vocabulary)[1000:1015]:\n","    print key, vectorizer.vocabulary[key]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IBBG_AQnqHBh"},"source":["And each column (word) has a unique tf-idf value."]},{"cell_type":"markdown","metadata":{"id":"bPsW1YS4qHBi"},"source":["I can therefore map the value of the content tokens to their tf-idf, using the `vectorizer.vocabulary` dictionary. "]},{"cell_type":"code","metadata":{"id":"hKdb_2xmqHBi"},"source":["for token in content_tweets[500]:\n","    print (token.lemma_, vectorizer.vocabulary[token.lemma_], np.max(np_matrix[:,vectorizer.vocabulary[token.lemma_]]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kNoOYVtHqHBj"},"source":["tfidf_dict = {}\n","content_vocab = []\n","for tweet in content_tweets: \n","    for token in tweet: \n","        if token.lemma_ not in tfidf_dict: \n","            content_vocab.append(token.lemma_)\n","            tfidf_dict[token.lemma_] = np.max(np_matrix[:,vectorizer.vocabulary[token.lemma_]])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"unKzecyOqHBj","scrolled":true},"source":["for key in sorted(tfidf_dict)[500:505]:\n","    print (\"WORD:\" + str(key) + \" -- tf-idf SCORE:\" +  str(tfidf_dict[key]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xT34P28jqHBk"},"source":["Success! "]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"NZrz0CbdqHBk"},"source":["## COntent Word-based Tweet Summarization (COWTS) \n","As per [Rudra et al](http://dl.acm.org/citation.cfm?id=2806485). \n","\n","I'll be using [PyMathProg](http://pymprog.sourceforge.net/index.html) as my Integer Linear Programming Solver. This is a python interface for [GLPK](https://www.gnu.org/software/glpk/)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":299},"id":"m3luZ-OCqHBl","executionInfo":{"status":"error","timestamp":1619116218135,"user_tz":-330,"elapsed":1119,"user":{"displayName":"shraddha bharadwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhDNQt9tAYdGTonkIlvAJWWwl57-aaTNZ2JOU6Mg=s64","userId":"13889782027816332292"}},"outputId":"1fb4a5c6-51b6-46bd-d3e7-be99f0471a93"},"source":["import pymprog "],"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-66-1ac5592ff22f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpymprog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pymprog'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"7PpnR372qHBl"},"source":["I want to maximize \n","\\begin{equation}\n","\\sum_{i=1}^n x_{i} + \\sum_{j = 1}^{m} Score(j) \\cdot y_{j}\n","\\end{equation}\n","Where $x_{i}$ is 1 if I include tweet i, or 0 if I don't, and where $y_{j}$ is 1 or 0 if each content word is included (and Score(j) is that word's tf-idf score). \n","\n","I'm going to subject this equation to the following constraints: \n","\n","1. \n","\\begin{equation}\n","\\sum_{i=1}^{n} x_{i} \\cdot Length(i) \\leq L\n","\\end{equation}\n","I want the total length of all the selected tweets to be less than some value L, which will be the length of my summary, L. I can vary L depending on how long I want my summary to be. \n","\n","2. \n","\\begin{equation}\n","\\sum_{i \\in T_{j}} x_{i} \\geq y_{j}, j = [1,...,m]\n","\\end{equation}\n","If I pick some content word $y_{j}$ (out of my $m$ possible content words) , then I want to have at least one tweet from the set of tweets which contain that content word, $T_{j}$. \n","\n","3. \n","\\begin{equation}\n","\\sum_{j \\in C_{i}} y_{j} \\leq |C_{i}| \\times x_{i}, i = [1,...,n]\n","\\end{equation}\n","If I pick some tweet i (out of my $n$ possible tweets) , then all the content words in that tweet $C_{i}$ are also selected. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":163},"id":"irJTJMpRqHBm","scrolled":true,"outputId":"0e463cda-9d23-49c1-ecdd-2bc96a38807f"},"source":["begin('COWTS')"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-12-4d26a9ba42b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'COWTS'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'begin' is not defined"]}]},{"cell_type":"code","metadata":{"id":"Z0yCmzLjqHBm"},"source":["# Defining my first variable, x \n","# This defines whether or not a tweet is selected\n","x = var('x', len(spacy_tweets), bool)\n","\n","# Check this worked\n","x[1000]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M-GhV3D3qHBn"},"source":["# Also defining the second variable, which defines\n","# whether or not a content word is chosen\n","y = var('y', len(content_vocab), bool)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bhYP2SAdqHBr"},"source":["len(y), y[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Du-ToZiUqHBs"},"source":["Now that I have defined my variables, I can define the equation I am maximizing. "]},{"cell_type":"code","metadata":{"id":"CDHC0yVTqHBt"},"source":["maximize(sum(x) + sum([tfidf_dict[content_vocab[j]]*y[j] for j in range(len(y))]));"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vv35c6azqHBt"},"source":["Now, I can define my constraints. First, \n","\\begin{equation}\n","\\sum_{i=1}^{n} x_{i} \\cdot Length(i) \\leq L\n","\\end{equation}"]},{"cell_type":"code","metadata":{"id":"jqd2Yrq_qHBu"},"source":["## Maximum length of the entire tweet summary\n","\n","# Was 150 for the tweet summary, \n","# But generated a 1000 word summary for CONABS\n","L = 1000\n","\n","# hiding the output of this line since its a very long sum \n","sum([x[i]*len(spacy_tweets[i]) for i in range(len(x))]) <= L;"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fLUwMT7EqHBu"},"source":["These next two constraints are slightly more tricky, as I need a way to define which content words are in which tweets. \n","\n","However, the term matrix I defined using the vectorizer has all of this information. \n","\n","I'll begin by defining two helper methods"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"7Va1lWbkqHBv"},"source":["def content_words(i):\n","    '''Given a tweet index i (for x[i]), this method will return the indices of the words in the \n","    content_vocab[] array\n","    Note: these indices are the same as for the y variable\n","    '''\n","    tweet = spacy_tweets[i]\n","    content_indices = []\n","    \n","    for token in tweet:\n","        if token.lemma_ in content_vocab:\n","            content_indices.append(content_vocab.index(token.lemma_))\n","    return content_indices"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"8ZsloegfqHBv"},"source":["def tweets_with_content_words(j):\n","    '''Given the index j of some content word (for content_vocab[j] or y[j])\n","    this method will return the indices of all tweets which contain this content word\n","    '''\n","    content_word = content_vocab[j]\n","    \n","    index_in_term_matrix = vectorizer.vocabulary[content_word]\n","    \n","    matrix_column = np_matrix[:, index_in_term_matrix]\n","    \n","    return np.nonzero(matrix_column)[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3gTmVIqIqHBw"},"source":["I can now define the second constraint: \n","\\begin{equation}\n","\\sum_{i \\in T_{j}} x_{i} \\geq y_{j}, j = [1,...,m]\n","\\end{equation}"]},{"cell_type":"code","metadata":{"id":"VYk_lluwqHBw"},"source":["for j in range(len(y)):\n","    sum([x[i] for i in tweets_with_content_words(j)])>= y[j]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d-EQMTKtqHBx"},"source":["And the third constraint:\n","\\begin{equation}\n","\\sum_{j \\in C_{i}} y_{j} \\leq |C_{i}| \\times x_{i}, i = [1,...,n]\n","\\end{equation}"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"eootQRLOqHBx"},"source":["for i in range(len(x)):\n","    sum(y[j] for j in content_words(i)) >= len(content_words(i))*x[i]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bzYFqkFGqHBy"},"source":["solve()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"83pFSo82qHBy"},"source":["result_x =  [value.primal for value in x]\n","result_y = [value.primal for value in y]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AFwHSY9AqHBz","scrolled":true},"source":["end()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"a5Uru9KuqHBz"},"source":["chosen_tweets = np.nonzero(result_x)\n","chosen_words = np.nonzero(result_y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s3a3bO8ZqHB0"},"source":["len(chosen_tweets[0]), len(chosen_words[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n9hCaxdUqHB0"},"source":["Lets take a look at the results! "]},{"cell_type":"code","metadata":{"id":"Zh8FQiEeqHB1"},"source":["for i in chosen_tweets[0]:\n","    print ('--------------')\n","    print spacy_tweets[i]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C02AtqHWqHB1"},"source":["There is definitely noise amongst these tweets, but these tweets do successfully provide a good overview of the situation in Nepal.\n","\n","I am going to compare this to random tweets, to make sure it does perform better than 16 randomly chosen tweets. "]},{"cell_type":"code","metadata":{"id":"x7kJ7rekqHB2"},"source":["random_tweets = np.random.choice(spacy_tweets, size=11)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ycfVNhpqHB2"},"source":["for i in random_tweets:\n","    print ('--------')\n","    print i"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"64xWBwYnqHB4"},"source":["A brief comparison does indicate that this method is far better than random choice at providing a situational overview. \n","\n","It's worth noting that even a random distribution will contain a fair amount of information, because of the selective nature in which we isolated tweets; this is already a subsample which contains a higher % of relevant information. "]},{"cell_type":"code","metadata":{"id":"6tTIg8WTqHB5"},"source":["cowts_tweets = []\n","for i in chosen_tweets[0]:\n","    cowts_tweets.append(spacy_tweets[i])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pDLRtrcXqHB6"},"source":["Lets take a look at the first few tweets"]},{"cell_type":"code","metadata":{"id":"xC5IKX4hqHB6"},"source":["for tweet in cowts_tweets[:10]:\n","    print ('--------')\n","    print tweet"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Prx5fzJkqHB7"},"source":["This notebook is getting long, so I'm going to save these tweets (which I will continue using) and start a fresh notebook for the next steps. \n","\n","## Saving everything for a fresh notebook "]},{"cell_type":"code","metadata":{"id":"2_dRq4H4qHB8"},"source":["cowts_unicode = [x.text for x in cowts_tweets]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"lgRnwGS_qHB8"},"source":["cowts_dataframe = pd.DataFrame(cowts_unicode)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LogJfCUWqHB8","outputId":"475b9702-15b8-41b2-d147-afffb44f0908"},"source":["cowts_dataframe.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style>\n","    .dataframe thead tr:only-child th {\n","        text-align: right;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: left;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>: LATEST Nepal's Kantipur TV shows at least 21...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Prayers for the affected people across SouthAs...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>: Due to bulding collaps 12 People died in Eas...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>: M7.9  - 29km ESE of Lamjung, Nepal  20 00 29...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Earthquake: 2015-04-25 17:30HKT M5.0 [28.0N,85...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                   0\n","0  : LATEST Nepal's Kantipur TV shows at least 21...\n","1  Prayers for the affected people across SouthAs...\n","2  : Due to bulding collaps 12 People died in Eas...\n","3  : M7.9  - 29km ESE of Lamjung, Nepal  20 00 29...\n","4  Earthquake: 2015-04-25 17:30HKT M5.0 [28.0N,85..."]},"metadata":{"tags":[]},"execution_count":169}]},{"cell_type":"markdown","metadata":{"id":"MhpqOHhKqHB9"},"source":["Saving it to a pickle: "]},{"cell_type":"code","metadata":{"id":"P-tKD1feqHB-"},"source":["cowts_dataframe.to_pickle('cowts_tweets.pkl')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"IJbcHROdqHB-"},"source":["np.save('term_matrix.npy', np_matrix)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"frVrqaWeqHB_"},"source":["np.save('tweet_indices.npy', chosen_tweets)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KdIojL2wqHB_"},"source":["np.save('vocab_to_idx.npy', vectorizer.vocabulary)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"cHy8qtcFqHB_"},"source":["np.save('content_vocab.npy', content_vocab)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"PpbfsbtQqHCA"},"source":["np.save('tfidf_dict.npy', tfidf_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"W2-i8DEtqHCA"},"source":[""],"execution_count":null,"outputs":[]}]}