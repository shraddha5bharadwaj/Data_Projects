{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.13"},"colab":{"name":"2 - Tweet Summarization.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"JJpwzHlgqHAe"},"source":["# Content Words"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qvxsK0da5E6G","executionInfo":{"status":"ok","timestamp":1619100206510,"user_tz":-330,"elapsed":11610,"user":{"displayName":"shraddha bharadwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhDNQt9tAYdGTonkIlvAJWWwl57-aaTNZ2JOU6Mg=s64","userId":"13889782027816332292"}},"outputId":"a5ed8684-f1cc-4ba5-f98d-53fc4d1302b7"},"source":["!apt-get -qq install -y libarchive-dev && pip install -U libarchive\n","import libarchive"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Selecting previously unselected package libarchive-dev:amd64.\n","(Reading database ... 160983 files and directories currently installed.)\n","Preparing to unpack .../libarchive-dev_3.2.2-3.1ubuntu0.6_amd64.deb ...\n","Unpacking libarchive-dev:amd64 (3.2.2-3.1ubuntu0.6) ...\n","Setting up libarchive-dev:amd64 (3.2.2-3.1ubuntu0.6) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Collecting libarchive\n","  Downloading https://files.pythonhosted.org/packages/bf/d4/26f5c9835d4d648e4f22b5fb91288457698e928aaf9d4ab7eff405b7ef03/libarchive-0.4.7.tar.gz\n","Requirement already satisfied, skipping upgrade: nose in /usr/local/lib/python2.7/dist-packages (from libarchive) (1.3.7)\n","Building wheels for collected packages: libarchive\n","  Building wheel for libarchive (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for libarchive: filename=libarchive-0.4.7-cp27-none-any.whl size=31634 sha256=7eefd4047f5a35256e9870a9881826bc4270fd8caa196ee742191602839b5bc8\n","  Stored in directory: /root/.cache/pip/wheels/3b/5c/fa/92ee330d259e8fa5bedbd53f67040710fe81cfa463b8711d26\n","Successfully built libarchive\n","Installing collected packages: libarchive\n","Successfully installed libarchive-0.4.7\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dn9vlQ3wsZiE","executionInfo":{"status":"ok","timestamp":1619108314765,"user_tz":-330,"elapsed":6610,"user":{"displayName":"shraddha bharadwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhDNQt9tAYdGTonkIlvAJWWwl57-aaTNZ2JOU6Mg=s64","userId":"13889782027816332292"}},"outputId":"ad08f7e1-410d-4589-b52b-af0063edd6ce"},"source":["!pip install spacy\n","!pip install textacy"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: spacy in /usr/local/lib/python2.7/dist-packages (2.1.6)\n","Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python2.7/dist-packages (from spacy) (2.0.1)\n","Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python2.7/dist-packages (from spacy) (0.2.2)\n","Requirement already satisfied: pathlib==1.0.1; python_version < \"3.4\" in /usr/local/lib/python2.7/dist-packages (from spacy) (1.0.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python2.7/dist-packages (from spacy) (2.23.0)\n","Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python2.7/dist-packages (from spacy) (0.9.6)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python2.7/dist-packages (from spacy) (2.0.2)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python2.7/dist-packages (from spacy) (1.16.4)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python2.7/dist-packages (from spacy) (1.0.2)\n","Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python2.7/dist-packages (from spacy) (0.0.7)\n","Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python2.7/dist-packages (from spacy) (0.2.4)\n","Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python2.7/dist-packages (from spacy) (7.0.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.6.16)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n","Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python2.7/dist-packages (from thinc<7.1.0,>=7.0.8->spacy) (4.28.1)\n","Requirement already satisfied: textacy in /usr/local/lib/python2.7/dist-packages (0.8.0)\n","Requirement already satisfied: scikit-learn<0.21.0,>=0.18.0 in /usr/local/lib/python2.7/dist-packages (from textacy) (0.20.3)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python2.7/dist-packages (from textacy) (1.2.2)\n","Requirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python2.7/dist-packages (from textacy) (0.14.1)\n","Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python2.7/dist-packages (from textacy) (2.2)\n","Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from textacy) (3.1.1)\n","Requirement already satisfied: numpy<2.0.0,>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from textacy) (1.16.4)\n","Requirement already satisfied: srsly>=0.0.5 in /usr/local/lib/python2.7/dist-packages (from textacy) (0.0.7)\n","Requirement already satisfied: backports.csv>=1.0.1; python_version == \"2.7\" in /usr/local/lib/python2.7/dist-packages (from textacy) (1.0.7)\n","Requirement already satisfied: pyemd>=0.3.0 in /usr/local/lib/python2.7/dist-packages (from textacy) (0.5.1)\n","Requirement already satisfied: cytoolz>=0.8.0 in /usr/local/lib/python2.7/dist-packages (from textacy) (0.11.0)\n","Requirement already satisfied: tqdm>=4.11.1 in /usr/local/lib/python2.7/dist-packages (from textacy) (4.28.1)\n","Requirement already satisfied: pyphen>=0.9.4 in /usr/local/lib/python2.7/dist-packages (from textacy) (0.10.0)\n","Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python2.7/dist-packages (from textacy) (2.23.0)\n","Requirement already satisfied: jellyfish<0.7.0,>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from textacy) (0.6.1)\n","Requirement already satisfied: spacy>=2.0.12 in /usr/local/lib/python2.7/dist-packages (from textacy) (2.1.6)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python2.7/dist-packages (from networkx>=1.11->textacy) (4.4.0)\n","Requirement already satisfied: pathlib==1.0.1; python_version < \"3.4\" in /usr/local/lib/python2.7/dist-packages (from srsly>=0.0.5->textacy) (1.0.1)\n","Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python2.7/dist-packages (from cytoolz>=0.8.0->textacy) (0.10.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests>=2.10.0->textacy) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests>=2.10.0->textacy) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests>=2.10.0->textacy) (2019.6.16)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests>=2.10.0->textacy) (2.8)\n","Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python2.7/dist-packages (from spacy>=2.0.12->textacy) (2.0.1)\n","Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python2.7/dist-packages (from spacy>=2.0.12->textacy) (0.2.2)\n","Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python2.7/dist-packages (from spacy>=2.0.12->textacy) (0.9.6)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python2.7/dist-packages (from spacy>=2.0.12->textacy) (2.0.2)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python2.7/dist-packages (from spacy>=2.0.12->textacy) (1.0.2)\n","Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python2.7/dist-packages (from spacy>=2.0.12->textacy) (0.2.4)\n","Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python2.7/dist-packages (from spacy>=2.0.12->textacy) (7.0.8)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R3fTA150Dr3b","executionInfo":{"status":"ok","timestamp":1619102998091,"user_tz":-330,"elapsed":4297,"user":{"displayName":"shraddha bharadwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhDNQt9tAYdGTonkIlvAJWWwl57-aaTNZ2JOU6Mg=s64","userId":"13889782027816332292"}},"outputId":"1df041c7-150d-41d8-d20d-2f56c1c66444"},"source":["!pip install functoolz"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Collecting functoolz\n","  Downloading https://files.pythonhosted.org/packages/8a/8b/b36a7fa5e7080e20e0838963d6ab6a3dea6ca6766b870421a5b6e8292899/functoolz-0.4.tar.gz\n","Building wheels for collected packages: functoolz\n","  Building wheel for functoolz (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for functoolz: filename=functoolz-0.4-cp27-none-any.whl size=3069 sha256=188919abfd32a42dc8fa5eee41eb2321be2ceab15264186acd6b4a9924bf3756\n","  Stored in directory: /root/.cache/pip/wheels/84/6b/7b/030b42288da092e557bfd48baf003daa5ac4dc18f53f21cd7d\n","Successfully built functoolz\n","Installing collected packages: functoolz\n","Successfully installed functoolz-0.4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":340},"id":"-70aCfi54-Lj","executionInfo":{"status":"error","timestamp":1619109711650,"user_tz":-330,"elapsed":977,"user":{"displayName":"shraddha bharadwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhDNQt9tAYdGTonkIlvAJWWwl57-aaTNZ2JOU6Mg=s64","userId":"13889782027816332292"}},"outputId":"68f0c0f3-1f4e-4fcc-829e-a0b959a1a44f"},"source":["import functoolz \n","\n","import textacy\n"],"execution_count":27,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-27-a44940493cba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctoolz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtextacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python2.7/dist-packages/textacy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_spacy_lang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_spacy_doc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtextacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_stats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextStats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspacier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_extensions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_doc_extensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python2.7/dist-packages/textacy/corpus.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msrsly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcytoolz\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitertoolz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthinc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneural\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNumpyOps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python2.7/dist-packages/cytoolz/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Always-curried functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mflip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctoolz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunctoolz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mmemoize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctoolz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunctoolz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'functoolz' is not defined"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rmUa4WkiqLyr","executionInfo":{"status":"ok","timestamp":1619100000485,"user_tz":-330,"elapsed":1276,"user":{"displayName":"shraddha bharadwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhDNQt9tAYdGTonkIlvAJWWwl57-aaTNZ2JOU6Mg=s64","userId":"13889782027816332292"}},"outputId":"e5f05bfe-8484-4227-baf6-0d81e0daeaca"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":374},"id":"jD_XwcvhqHA8","executionInfo":{"status":"error","timestamp":1619102428840,"user_tz":-330,"elapsed":1278,"user":{"displayName":"shraddha bharadwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhDNQt9tAYdGTonkIlvAJWWwl57-aaTNZ2JOU6Mg=s64","userId":"13889782027816332292"}},"outputId":"e1e90c02-b53e-46de-c928-a4eca4867c15"},"source":["% matplotlib inline\n","\n","from __future__ import division\n","\n","import numpy as np \n","import pandas as pd \n","\n","import spacy\n","from spacy.tokens.doc import Doc\n","import inspect\n","import textacy\n","from textacy.vsm import Vectorizer\n","import textacy.vsm\n","\n","import scipy.sparse as sp\n","\n","from tqdm import *\n","\n","import re"],"execution_count":10,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-10-ddef23579cde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtextacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvsm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtextacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvsm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python2.7/dist-packages/textacy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_spacy_lang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_spacy_doc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtextacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_stats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextStats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspacier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_extensions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_doc_extensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python2.7/dist-packages/textacy/corpus.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msrsly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcytoolz\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitertoolz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthinc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneural\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNumpyOps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python2.7/dist-packages/cytoolz/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Always-curried functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mflip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctoolz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunctoolz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mmemoize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctoolz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunctoolz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'functoolz' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"KlmvZM91qHBA"},"source":["Loading the data"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"lb3wLzsOqHBC"},"source":["tweets = pd.read_csv('tweet_ids/2015_Nepal_Earthquake_en/stripped_filled_tweets.csv', encoding = 'ISO-8859-1')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"syFPpdryvYRs"},"source":["tweets = pd.read_csv('/content/drive/My Drive/data/filled_tweets.csv', encoding = 'ISO-8859-1')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ur38zpmvqHBD","outputId":"ec5bcf02-5a1d-41c6-f7f0-335603c73139"},"source":["tweets.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style>\n","    .dataframe thead tr:only-child th {\n","        text-align: right;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: left;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>label</th>\n","      <th>tweet_id</th>\n","      <th>tweet_texts</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>infrastructure_and_utilities_damage</td>\n","      <td>'591902695822331904'</td>\n","      <td>RT @DailySabah: #LATEST #Nepal's Kantipur TV s...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>injured_or_dead_people</td>\n","      <td>'591902695943843840'</td>\n","      <td>RT @iamsrk: May Allah look after all. Here r t...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>missing_trapped_or_found_people</td>\n","      <td>'591902696371724288'</td>\n","      <td>RT @RT_com: LATEST: 108 killed in 7.9-magnitud...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>sympathy_and_emotional_support</td>\n","      <td>'591902696375877632'</td>\n","      <td>RT @Edourdoo: Shocking picture of the earthqua...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>sympathy_and_emotional_support</td>\n","      <td>'591902696895950848'</td>\n","      <td>Indian Air Force is ready to help the people o...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Unnamed: 0                                label              tweet_id  \\\n","0          1  infrastructure_and_utilities_damage  '591902695822331904'   \n","1          2               injured_or_dead_people  '591902695943843840'   \n","2          3      missing_trapped_or_found_people  '591902696371724288'   \n","3          4       sympathy_and_emotional_support  '591902696375877632'   \n","4          5       sympathy_and_emotional_support  '591902696895950848'   \n","\n","                                         tweet_texts  \n","0  RT @DailySabah: #LATEST #Nepal's Kantipur TV s...  \n","1  RT @iamsrk: May Allah look after all. Here r t...  \n","2  RT @RT_com: LATEST: 108 killed in 7.9-magnitud...  \n","3  RT @Edourdoo: Shocking picture of the earthqua...  \n","4  Indian Air Force is ready to help the people o...  "]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"fqVJN4v2vxXw","executionInfo":{"status":"ok","timestamp":1619097764279,"user_tz":-330,"elapsed":1217,"user":{"displayName":"shraddha bharadwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhDNQt9tAYdGTonkIlvAJWWwl57-aaTNZ2JOU6Mg=s64","userId":"13889782027816332292"}},"outputId":"cdc963e7-b099-41cd-dd42-25ff57e8583e"},"source":["tweets.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["  Unnamed: 0                                label              tweet_id  \\\n","0          0             other_useful_information  '591902695562170368'   \n","1          1  infrastructure_and_utilities_damage  '591902695822331904'   \n","2          2               injured_or_dead_people  '591902695943843840'   \n","3          3      missing_trapped_or_found_people  '591902696371724288'   \n","4          4       sympathy_and_emotional_support  '591902696375877632'   \n","\n","                                         tweet_texts  \n","0                                                NaN  \n","1  RT @DailySabah: #LATEST #Nepal's Kantipur TV s...  \n","2  RT @iamsrk: May Allah look after all. Here r t...  \n","3  RT @RT_com: LATEST: 108 killed in 7.9-magnitud...  \n","4  RT @Edourdoo: Shocking picture of the earthqua...  "],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>label</th>\n","      <th>tweet_id</th>\n","      <th>tweet_texts</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>other_useful_information</td>\n","      <td>'591902695562170368'</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>infrastructure_and_utilities_damage</td>\n","      <td>'591902695822331904'</td>\n","      <td>RT @DailySabah: #LATEST #Nepal's Kantipur TV s...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>injured_or_dead_people</td>\n","      <td>'591902695943843840'</td>\n","      <td>RT @iamsrk: May Allah look after all. Here r t...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>missing_trapped_or_found_people</td>\n","      <td>'591902696371724288'</td>\n","      <td>RT @RT_com: LATEST: 108 killed in 7.9-magnitud...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>sympathy_and_emotional_support</td>\n","      <td>'591902696375877632'</td>\n","      <td>RT @Edourdoo: Shocking picture of the earthqua...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"PQylD4sgqHBG"},"source":["tweets = tweets.dropna()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rFxainNhqHBH"},"source":["## Preprocessing\n","\n","For my tweets to be informative, there are a few terms I can immediately remove. For instance, any urls won't be useful to the rescue teams. Equally, any '@...' are just calling another twitter handle, and are equally not useful. "]},{"cell_type":"code","metadata":{"id":"GzKlwQlmqHBJ"},"source":["# removing URLS\n","tweets.tweet_texts = tweets.tweet_texts.apply(lambda x: re.sub(u'http\\S+', u'', x))   \n","\n","# removing @... \n","tweets.tweet_texts = tweets.tweet_texts.apply(lambda x: re.sub(u'(\\s)@\\w+', u'', x))\n","\n","# removing hashtags\n","tweets.tweet_texts = tweets.tweet_texts.apply(lambda x: re.sub(u'#', u'', x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JwdQ8nR7qHBK","outputId":"d45ade2b-1379-4cb7-ad03-ddd0caa6d99c"},"source":["tweets.tweet_texts.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    RT: LATEST Nepal's Kantipur TV shows at least ...\n","1    RT: May Allah look after all. Here r the emerg...\n","2    RT: LATEST: 108 killed in 7.9-magnitude Nepal ...\n","3    RT: Shocking picture of the earthquake in Nepa...\n","4    Indian Air Force is ready to help the people o...\n","Name: tweet_texts, dtype: object"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"Vh23a6z9qHBL"},"source":["There are alot of `u'RT'` terms in the tweet texts. Since this isn't a word, SpaCy doesn't know how to handle them. Since these add nothing to the content of a tweet, I'm just going to get rid of them. "]},{"cell_type":"code","metadata":{"collapsed":true,"id":"oydH5JJ1qHBN"},"source":["tweets.tweet_texts = tweets.tweet_texts.apply(lambda x: x.replace(u'RT', u''))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"913TTHaIqHBO"},"source":["Tokenizing with SpaCy"]},{"cell_type":"code","metadata":{"id":"Kz62_Y3qqHBO"},"source":["nlp = spacy.load('en')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DLsJEFqkqHBP"},"source":["spacy_tweets = []\n","\n","for doc in nlp.pipe(tweets.tweet_texts, n_threads = -1):\n","    spacy_tweets.append(doc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iRHjf3CzqHBP","outputId":"6811e528-2984-4fd8-d67e-89dfca86a3a9"},"source":["spacy_tweets[200]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MEA opens 24 hour Control Room for queries regarding the Nepal Earthquake.åÊ\n","Numbers:\n","+91 11 2301 2113\n","+91 11 2301 4104\n","+91 11 2301 7905"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"NIJFmc0EqHBQ"},"source":["Not all tweets are equally useful. Some just contain prayers, such as\n","\n","`Hope it doesn't rain. #Nepal`\n","\n","whereas others are dense with useful information: \n","\n","`2 Dead, 100 Injured in Bangladesh From Nepal Quake`"]},{"cell_type":"markdown","metadata":{"id":"wJK0B53LqHBR"},"source":["How do I decide which parts of these tweets are most useful? One way to do it is to measure the term frequency-inverse document frequency (tf-idf) of each of the words in the corpus of tweets. This metric measures how important a word is in a corpus of tweets. \n","\n","## Getting the tf-idf values of content words. \n","\n","I can do a preliminary 'cleanup', by keeping only 'content words'. These are defined as : Numerals, Nouns and Verbs. Conveniantly, SpaCy has already organised this for us. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":163},"id":"53QYQ85aqHBS","executionInfo":{"status":"error","timestamp":1619097961191,"user_tz":-330,"elapsed":956,"user":{"displayName":"shraddha bharadwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhDNQt9tAYdGTonkIlvAJWWwl57-aaTNZ2JOU6Mg=s64","userId":"13889782027816332292"}},"outputId":"a7e5beb2-686e-4fbd-c489-3ca17660ad7b"},"source":["spacy_tweets[90]"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-7-9a80e2dae845>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspacy_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'spacy_tweets' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"MYtZ8E8rqHBT"},"source":["I care most about tokens which are entities, and numbers. The other tokens have too much noise, so let's focus on these two:"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"ofRuMKqWqHBT"},"source":["main_words = [u'earthquake', u'killed', u'injured', u'stranded', u'wounded', u'hurt', u'helpless', u'wrecked', u'nepal'] # get this from our dataset with target 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"8CP8J_8iqHBU"},"source":["useful_entities = [u'NORP', u'FACILITY', u'ORG', u'GPE', u'LOC', u'EVENT', u'DATE', u'TIME']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":231},"id":"9QkVG0wWqHBV","executionInfo":{"status":"error","timestamp":1619097962183,"user_tz":-330,"elapsed":1859,"user":{"displayName":"shraddha bharadwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhDNQt9tAYdGTonkIlvAJWWwl57-aaTNZ2JOU6Mg=s64","userId":"13889782027816332292"}},"outputId":"c3100883-8373-4375-e61f-6576a9e64d9c"},"source":["content_tweets = []\n","for single_tweet in tqdm(spacy_tweets):\n","    single_tweet_content = []\n","    for token in single_tweet: \n","        if ((token.ent_type_ in useful_entities)  \n","            or (token.pos_ == u'NUM') \n","            or (token.lower_ in main_words)):\n","            single_tweet_content.append(token)\n","    content_tweets.append(single_tweet_content)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-10-39bca06bbbcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcontent_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msingle_tweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspacy_tweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0msingle_tweet_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msingle_tweet\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         if ((token.ent_type_ in useful_entities)  \n","\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"]}]},{"cell_type":"code","metadata":{"id":"fNqkjb0YqHBW"},"source":["tweet_num = 200\n","print (\"original_tweet \\n\" + str(spacy_tweets[tweet_num]) \n","       + \"\\n\\noriginal_tweet\\n\" + str([str(x) for x in spacy_tweets[tweet_num]])\n","       + \"\\n\\ncontent_tweet\\n\" + str(content_tweets[tweet_num])\n","      )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qAnrAfKqqHBX"},"source":["So this has already gone some way to (crudely) isolating the interesting parts of a tweet. "]},{"cell_type":"markdown","metadata":{"id":"ybxaWR5fqHBY"},"source":["Unfortunately, SpaCy doesn't calculate tf-idf score automatically. There IS a library which can do this: [textacy](https://textacy.readthedocs.io/en/latest/index.html). Note: textacy is built on SpaCy.\n","\n","I care about the tf-idf scores of the entire tweet, so will find the tf-idf score across the entire corpus of original tweets. "]},{"cell_type":"code","metadata":{"id":"rdKw0d-XqHBZ"},"source":["vectorizer = Vectorizer(weighting = 'tfidf')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8MVTChyEqHBa"},"source":["To calculate the tf-idf score of all the tokens in the tweets, I can use `fit_transform()`. \n","\n","Note: I am using the `lemma_` attribute of each token, because tokens contain information about the documents. This means that 'Nepal' in the 100th tweet will have a different **token** from 'Nepal' in the 200th tweet, but the same `lemma__` attribute. This is what I want to compare - I don't want hundreds of 'Nepal' columns in my term matrix. "]},{"cell_type":"code","metadata":{"id":"1tHN1V3wqHBb"},"source":["term_matrix = vectorizer.fit_transform([tok.lemma_ for tok in doc] for doc in spacy_tweets)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w4kRvAuqqHBc"},"source":["This matrix is a term-document matrix. What this means is that on top of having the tf-idf values, each row is a document (and each column is a word). \n","\n","If the tweet in row `i` contains the column in row `j`, then the element `matrix[i][j]` will contain the tf-idf value. If the tweet *doesn't* contan the word, the matrix value will be zero. "]},{"cell_type":"code","metadata":{"collapsed":true,"id":"xMA6DYuXqHBd"},"source":["np_matrix = term_matrix.todense()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ybFaSltbqHBd"},"source":["np_matrix.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"tVQwnqr_qHBe"},"source":["My ultimate goal is to create a dictionary, which maps from the tokens in the content tweets to some tf-idf score. To do this, I need to find out which tokens are at what columns in the term matrix. \n","\n","The vectorizer object has a dictionary, which maps token.lemma_ to its column. "]},{"cell_type":"code","metadata":{"id":"7MTd3J3MqHBg"},"source":["for key in sorted(vectorizer.vocabulary)[1000:1015]:\n","    print key, vectorizer.vocabulary[key]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IBBG_AQnqHBh"},"source":["And each column (word) has a unique tf-idf value."]},{"cell_type":"markdown","metadata":{"id":"bPsW1YS4qHBi"},"source":["I can therefore map the value of the content tokens to their tf-idf, using the `vectorizer.vocabulary` dictionary. "]},{"cell_type":"code","metadata":{"id":"hKdb_2xmqHBi"},"source":["for token in content_tweets[500]:\n","    print (token.lemma_, vectorizer.vocabulary[token.lemma_], np.max(np_matrix[:,vectorizer.vocabulary[token.lemma_]]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kNoOYVtHqHBj"},"source":["tfidf_dict = {}\n","content_vocab = []\n","for tweet in content_tweets: \n","    for token in tweet: \n","        if token.lemma_ not in tfidf_dict: \n","            content_vocab.append(token.lemma_)\n","            tfidf_dict[token.lemma_] = np.max(np_matrix[:,vectorizer.vocabulary[token.lemma_]])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"unKzecyOqHBj"},"source":["for key in sorted(tfidf_dict)[500:505]:\n","    print (\"WORD:\" + str(key) + \" -- tf-idf SCORE:\" +  str(tfidf_dict[key]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xT34P28jqHBk"},"source":["Success! "]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"NZrz0CbdqHBk"},"source":["## COntent Word-based Tweet Summarization (COWTS) \n","As per [Rudra et al](http://dl.acm.org/citation.cfm?id=2806485). \n","\n","I'll be using [PyMathProg](http://pymprog.sourceforge.net/index.html) as my Integer Linear Programming Solver. This is a python interface for [GLPK](https://www.gnu.org/software/glpk/)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":299},"id":"m3luZ-OCqHBl","executionInfo":{"status":"error","timestamp":1619098572842,"user_tz":-330,"elapsed":1125,"user":{"displayName":"shraddha bharadwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhDNQt9tAYdGTonkIlvAJWWwl57-aaTNZ2JOU6Mg=s64","userId":"13889782027816332292"}},"outputId":"278b6cd6-c268-4c37-9793-9d48a4ff73ec"},"source":["from pymprog import *"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-11-4f4bc8cb1527>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpymprog\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mImportError\u001b[0m: No module named pymprog","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"7PpnR372qHBl"},"source":["I want to maximize \n","\\begin{equation}\n","\\sum_{i=1}^n x_{i} + \\sum_{j = 1}^{m} Score(j) \\cdot y_{j}\n","\\end{equation}\n","Where $x_{i}$ is 1 if I include tweet i, or 0 if I don't, and where $y_{j}$ is 1 or 0 if each content word is included (and Score(j) is that word's tf-idf score). \n","\n","I'm going to subject this equation to the following constraints: \n","\n","1. \n","\\begin{equation}\n","\\sum_{i=1}^{n} x_{i} \\cdot Length(i) \\leq L\n","\\end{equation}\n","I want the total length of all the selected tweets to be less than some value L, which will be the length of my summary, L. I can vary L depending on how long I want my summary to be. \n","\n","2. \n","\\begin{equation}\n","\\sum_{i \\in T_{j}} x_{i} \\geq y_{j}, j = [1,...,m]\n","\\end{equation}\n","If I pick some content word $y_{j}$ (out of my $m$ possible content words) , then I want to have at least one tweet from the set of tweets which contain that content word, $T_{j}$. \n","\n","3. \n","\\begin{equation}\n","\\sum_{j \\in C_{i}} y_{j} \\leq |C_{i}| \\times x_{i}, i = [1,...,n]\n","\\end{equation}\n","If I pick some tweet i (out of my $n$ possible tweets) , then all the content words in that tweet $C_{i}$ are also selected. "]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":163},"id":"irJTJMpRqHBm","executionInfo":{"status":"error","timestamp":1619098574898,"user_tz":-330,"elapsed":3142,"user":{"displayName":"shraddha bharadwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhhDNQt9tAYdGTonkIlvAJWWwl57-aaTNZ2JOU6Mg=s64","userId":"13889782027816332292"}},"outputId":"0e463cda-9d23-49c1-ecdd-2bc96a38807f"},"source":["begin('COWTS')"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-12-4d26a9ba42b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'COWTS'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'begin' is not defined"]}]},{"cell_type":"code","metadata":{"id":"Z0yCmzLjqHBm"},"source":["# Defining my first variable, x \n","# This defines whether or not a tweet is selected\n","x = var('x', len(spacy_tweets), bool)\n","\n","# Check this worked\n","x[1000]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M-GhV3D3qHBn"},"source":["# Also defining the second variable, which defines\n","# whether or not a content word is chosen\n","y = var('y', len(content_vocab), bool)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bhYP2SAdqHBr"},"source":["len(y), y[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Du-ToZiUqHBs"},"source":["Now that I have defined my variables, I can define the equation I am maximizing. "]},{"cell_type":"code","metadata":{"id":"CDHC0yVTqHBt"},"source":["maximize(sum(x) + sum([tfidf_dict[content_vocab[j]]*y[j] for j in range(len(y))]));"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vv35c6azqHBt"},"source":["Now, I can define my constraints. First, \n","\\begin{equation}\n","\\sum_{i=1}^{n} x_{i} \\cdot Length(i) \\leq L\n","\\end{equation}"]},{"cell_type":"code","metadata":{"id":"jqd2Yrq_qHBu"},"source":["## Maximum length of the entire tweet summary\n","\n","# Was 150 for the tweet summary, \n","# But generated a 1000 word summary for CONABS\n","L = 1000\n","\n","# hiding the output of this line since its a very long sum \n","sum([x[i]*len(spacy_tweets[i]) for i in range(len(x))]) <= L;"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fLUwMT7EqHBu"},"source":["These next two constraints are slightly more tricky, as I need a way to define which content words are in which tweets. \n","\n","However, the term matrix I defined using the vectorizer has all of this information. \n","\n","I'll begin by defining two helper methods"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"7Va1lWbkqHBv"},"source":["def content_words(i):\n","    '''Given a tweet index i (for x[i]), this method will return the indices of the words in the \n","    content_vocab[] array\n","    Note: these indices are the same as for the y variable\n","    '''\n","    tweet = spacy_tweets[i]\n","    content_indices = []\n","    \n","    for token in tweet:\n","        if token.lemma_ in content_vocab:\n","            content_indices.append(content_vocab.index(token.lemma_))\n","    return content_indices"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"8ZsloegfqHBv"},"source":["def tweets_with_content_words(j):\n","    '''Given the index j of some content word (for content_vocab[j] or y[j])\n","    this method will return the indices of all tweets which contain this content word\n","    '''\n","    content_word = content_vocab[j]\n","    \n","    index_in_term_matrix = vectorizer.vocabulary[content_word]\n","    \n","    matrix_column = np_matrix[:, index_in_term_matrix]\n","    \n","    return np.nonzero(matrix_column)[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3gTmVIqIqHBw"},"source":["I can now define the second constraint: \n","\\begin{equation}\n","\\sum_{i \\in T_{j}} x_{i} \\geq y_{j}, j = [1,...,m]\n","\\end{equation}"]},{"cell_type":"code","metadata":{"id":"VYk_lluwqHBw"},"source":["for j in range(len(y)):\n","    sum([x[i] for i in tweets_with_content_words(j)])>= y[j]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d-EQMTKtqHBx"},"source":["And the third constraint:\n","\\begin{equation}\n","\\sum_{j \\in C_{i}} y_{j} \\leq |C_{i}| \\times x_{i}, i = [1,...,n]\n","\\end{equation}"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"eootQRLOqHBx"},"source":["for i in range(len(x)):\n","    sum(y[j] for j in content_words(i)) >= len(content_words(i))*x[i]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bzYFqkFGqHBy"},"source":["solve()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"83pFSo82qHBy"},"source":["result_x =  [value.primal for value in x]\n","result_y = [value.primal for value in y]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"AFwHSY9AqHBz"},"source":["end()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"a5Uru9KuqHBz"},"source":["chosen_tweets = np.nonzero(result_x)\n","chosen_words = np.nonzero(result_y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s3a3bO8ZqHB0"},"source":["len(chosen_tweets[0]), len(chosen_words[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n9hCaxdUqHB0"},"source":["Lets take a look at the results! "]},{"cell_type":"code","metadata":{"id":"Zh8FQiEeqHB1"},"source":["for i in chosen_tweets[0]:\n","    print ('--------------')\n","    print spacy_tweets[i]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C02AtqHWqHB1"},"source":["There is definitely noise amongst these tweets, but these tweets do successfully provide a good overview of the situation in Nepal.\n","\n","I am going to compare this to random tweets, to make sure it does perform better than 16 randomly chosen tweets. "]},{"cell_type":"code","metadata":{"id":"x7kJ7rekqHB2"},"source":["random_tweets = np.random.choice(spacy_tweets, size=11)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ycfVNhpqHB2"},"source":["for i in random_tweets:\n","    print ('--------')\n","    print i"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"64xWBwYnqHB4"},"source":["A brief comparison does indicate that this method is far better than random choice at providing a situational overview. \n","\n","It's worth noting that even a random distribution will contain a fair amount of information, because of the selective nature in which we isolated tweets; this is already a subsample which contains a higher % of relevant information. "]},{"cell_type":"code","metadata":{"id":"6tTIg8WTqHB5"},"source":["cowts_tweets = []\n","for i in chosen_tweets[0]:\n","    cowts_tweets.append(spacy_tweets[i])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pDLRtrcXqHB6"},"source":["Lets take a look at the first few tweets"]},{"cell_type":"code","metadata":{"id":"xC5IKX4hqHB6"},"source":["for tweet in cowts_tweets[:10]:\n","    print ('--------')\n","    print tweet"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Prx5fzJkqHB7"},"source":["This notebook is getting long, so I'm going to save these tweets (which I will continue using) and start a fresh notebook for the next steps. \n","\n","## Saving everything for a fresh notebook "]},{"cell_type":"code","metadata":{"id":"2_dRq4H4qHB8"},"source":["cowts_unicode = [x.text for x in cowts_tweets]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"lgRnwGS_qHB8"},"source":["cowts_dataframe = pd.DataFrame(cowts_unicode)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LogJfCUWqHB8","outputId":"475b9702-15b8-41b2-d147-afffb44f0908"},"source":["cowts_dataframe.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style>\n","    .dataframe thead tr:only-child th {\n","        text-align: right;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: left;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>: LATEST Nepal's Kantipur TV shows at least 21...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Prayers for the affected people across SouthAs...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>: Due to bulding collaps 12 People died in Eas...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>: M7.9  - 29km ESE of Lamjung, Nepal  20 00 29...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Earthquake: 2015-04-25 17:30HKT M5.0 [28.0N,85...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                   0\n","0  : LATEST Nepal's Kantipur TV shows at least 21...\n","1  Prayers for the affected people across SouthAs...\n","2  : Due to bulding collaps 12 People died in Eas...\n","3  : M7.9  - 29km ESE of Lamjung, Nepal  20 00 29...\n","4  Earthquake: 2015-04-25 17:30HKT M5.0 [28.0N,85..."]},"metadata":{"tags":[]},"execution_count":169}]},{"cell_type":"markdown","metadata":{"id":"MhpqOHhKqHB9"},"source":["Saving it to a pickle: "]},{"cell_type":"code","metadata":{"id":"P-tKD1feqHB-"},"source":["cowts_dataframe.to_pickle('cowts_tweets.pkl')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"IJbcHROdqHB-"},"source":["np.save('term_matrix.npy', np_matrix)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"frVrqaWeqHB_"},"source":["np.save('tweet_indices.npy', chosen_tweets)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KdIojL2wqHB_"},"source":["np.save('vocab_to_idx.npy', vectorizer.vocabulary)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"cHy8qtcFqHB_"},"source":["np.save('content_vocab.npy', content_vocab)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"PpbfsbtQqHCA"},"source":["np.save('tfidf_dict.npy', tfidf_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"W2-i8DEtqHCA"},"source":[""],"execution_count":null,"outputs":[]}]}